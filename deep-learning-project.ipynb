{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qRtuZENww0d"
   },
   "source": [
    "# 0. Imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UOlpf5IRws-i"
   },
   "outputs": [],
   "source": [
    "# for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for data normalization and splitting\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for models\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import callbacks\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras_tuner\n",
    "\n",
    "# for plots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3Sj5NJ6rz2-"
   },
   "source": [
    "# 1. Next Value Prediction Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ra_xitvr2Lhq"
   },
   "source": [
    "## 1.1 Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdM8h32s2Rr3"
   },
   "source": [
    "### 1.1.1 <code>features_and_lables</code>\n",
    "Given the normalized data, the function will produce sequencies of specific window size window shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UtyirwKy2LIi"
   },
   "outputs": [],
   "source": [
    "def features_and_labels(data, window_size: int = 30, window_shift: int = 6):\n",
    "  \"\"\"\n",
    "  Given the data, the function will produce sequencies of specific\n",
    "  window size window shift.\n",
    "  \"\"\"\n",
    "  features = []\n",
    "  labels = []\n",
    "\n",
    "  for i in range(window_size, len(data), window_shift):\n",
    "    features.append(np.reshape(data[i - window_size:i], (1, -1))[0])\n",
    "    labels.append(data[i])\n",
    "  features, labels = np.array(features), np.array(labels)\n",
    "\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHQyzZ7X5W5q"
   },
   "source": [
    "### 1.1.2 <code>build_model</code>\n",
    "Given in input the features, the model will be fitted to the input shape. Moreover, this model will have a single output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AbIsuTz55obV"
   },
   "outputs": [],
   "source": [
    "def build_model(features):\n",
    "  \"\"\"\n",
    "  Given in input the features, the model will be fitted to the input shape.\n",
    "  Moreover, this model will have a single output node.\n",
    "  Il meno peggio per ora (80,80,80,80,64,64,64,32,1)\n",
    "  \"\"\"\n",
    "\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Input((features.shape[1], 1), name='Input'))\n",
    "  model.add(layers.LSTM(units=80, return_sequences=True, name='LSTM-1'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=80, return_sequences=True, name='LSTM-2'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=80, name='LSTM-3'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=80, activation='relu', name='Dense-1'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=64, activation='relu', name='Dense-2'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=64, activation='relu', name='Dense-3'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=64, activation='relu', name='Dense-4'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=32, activation='relu', name='Dense-5'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=1, name='Dense-final'))\n",
    "  model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss=losses.mse, metrics=[metrics.mae])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4m1hRoOCtVB"
   },
   "source": [
    "### 1.1.3 <code>denormalize</code>\n",
    "Given in input a value and the scaler used for the normalization, the function returns the denormalized value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AKlKnriQC7FC"
   },
   "outputs": [],
   "source": [
    "def denormalize(value: float, scaler: StandardScaler):\n",
    "  \"\"\"\n",
    "  Given in input the mean absolute error and the scaler used for the\n",
    "  normalization, the function returns the denormalized value\n",
    "  \"\"\"\n",
    "  return (scaler.inverse_transform(np.reshape(np.array([value]), (-1, 1))))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6JayVHUVqPE"
   },
   "source": [
    "### 1.1.4 <code>model_builder</code>\n",
    "The function is used for searching the best model to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "49I2DzEQtSak"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  \"\"\"\n",
    "  The function is used for searching the best model to be used\n",
    "  \"\"\"\n",
    "\n",
    "  hp_units_1 = hp.Int('units_1', min_value=32, max_value=256, step=16)  # for LSTMs\n",
    "  hp_units_2 = hp.Int('units_2', min_value=32, max_value=256, step=16)  # for first dense\n",
    "  hp_units_3 = hp.Int('units_3', min_value=32, max_value=256, step=16)  # for second dense\n",
    "  hp_units_4 = hp.Int('units_4', min_value=32, max_value=256, step=16)  # for third dense\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Input((30, 1), name='Input'))\n",
    "  model.add(layers.LSTM(units=hp_units_1, return_sequences=True, name='LSTM-1'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=hp_units_1, return_sequences=True, name='LSTM-2'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=hp_units_1, name='LSTM-3'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=hp_units_2, activation='relu', name='Dense-1'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=hp_units_3, activation='relu', name='Dense-2'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=hp_units_4, activation='relu', name='Dense-3'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=1, name='Dense-final'))\n",
    "  model.compile(optimizer=optimizers.Adam(learning_rate=hp_learning_rate), loss=losses.mse, metrics=[metrics.mae])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UZ_peltsBjo"
   },
   "source": [
    "## 1.2 Fixed windows properties\n",
    "We have 3 time series (X, Y, Z) recorded each 10 seconds\n",
    "- Given a sequence of 5 minutes every one minute\n",
    "- The goal is to predict the next value of the sequence for each time series\n",
    "- Evaluation Metric: **Mean Absolute Error**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87a7SVmA2G2s"
   },
   "source": [
    "### 1.2.1 Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UJ0l_SV7TftM",
    "outputId": "0ce36823-7ed1-40a3-b76a-f4e1b50f7435"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-24</td>\n",
       "      <td>749</td>\n",
       "      <td>-626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-206</td>\n",
       "      <td>930</td>\n",
       "      <td>-63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-139</td>\n",
       "      <td>763</td>\n",
       "      <td>-577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-503</td>\n",
       "      <td>441</td>\n",
       "      <td>-557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-278</td>\n",
       "      <td>705</td>\n",
       "      <td>-396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x    y    z\n",
       "0  -24  749 -626\n",
       "1 -206  930  -63\n",
       "2 -139  763 -577\n",
       "3 -503  441 -557\n",
       "4 -278  705 -396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataset_task_1/train.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihyHMqvJ6j3S"
   },
   "source": [
    "### 1.2.2 Reshaping columns\n",
    "We separate each column, to process them in a separate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiMyfWTBei14",
    "outputId": "0336abf7-de91-4181-bac9-473b6755c03b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -24],\n",
       "       [-206],\n",
       "       [-139],\n",
       "       ...,\n",
       "       [ 559],\n",
       "       [ 559],\n",
       "       [ 559]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.reshape(data.x.to_numpy(), (-1, 1))\n",
    "y_data = np.reshape(data.y.to_numpy(), (-1, 1))\n",
    "z_data = np.reshape(data.z.to_numpy(), (-1, 1))\n",
    "\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwReU1Sq7YMu"
   },
   "source": [
    "### 1.2.3 Data preprocessing\n",
    "We use the function <code>features_and_lables</code>, defined before, that helps with the transformation of data in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r97zIpuwhGwA",
    "outputId": "d8cc9346-f1c7-4a3c-8d70-ed73b98b68c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -24 -206 -139 -503 -278  240 -671  -45  102   15  142    3 -176 -768\n",
      " -838 -420 -419 -420 -419 -418 -417 -414 -408 -407 -407 -408 -407 -408\n",
      " -408 -409]\n",
      "[-408]\n"
     ]
    }
   ],
   "source": [
    "x_features, x_labels = features_and_labels(x_data, window_size=30, window_shift=6)\n",
    "y_features, y_labels = features_and_labels(y_data, window_size=30, window_shift=6)\n",
    "z_features, z_labels = features_and_labels(z_data, window_size=30, window_shift=6)\n",
    "\n",
    "print(x_features[0])\n",
    "print(x_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkRYB4Lv7AWv"
   },
   "source": [
    "### 1.2.4 Data normalization\n",
    "We use the standard scaler, in order to normalize values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__Gmr4shfOV3",
    "outputId": "4519f452-2ce7-489c-b9aa-e4f20655e870"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31846375, -0.69210742, -0.55333931, -1.29009094, -0.8330969 ,\n",
       "        0.21812059, -1.63390757, -0.36431967, -0.06244556, -0.23752702,\n",
       "        0.01981392, -0.26366122, -0.62766428, -1.83661042, -1.97735234,\n",
       "       -1.12156197, -1.11954163, -1.12352781, -1.12177456, -1.12407862,\n",
       "       -1.11992643, -1.10946296, -1.09729569, -1.09719701, -1.09747076,\n",
       "       -1.10380867, -1.09964788, -1.09736257, -1.09738732, -1.10135407])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(x_features)\n",
    "x_features = x_scaler.transform(x_features)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y_features)\n",
    "y_features = y_scaler.transform(y_features)\n",
    "\n",
    "z_scaler = StandardScaler()\n",
    "z_scaler.fit(z_features)\n",
    "z_features = z_scaler.transform(z_features)\n",
    "\n",
    "x_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3oELnj674AD"
   },
   "source": [
    "### 1.2.5 Splitting in training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "An0ypfE3iwYC"
   },
   "outputs": [],
   "source": [
    "x_train_features, x_val_features, x_train_labels, x_val_labels = train_test_split(x_features, x_labels, test_size=0.2, random_state=7)\n",
    "y_train_features, y_val_features, y_train_labels, y_val_labels = train_test_split(y_features, y_labels, test_size=0.2, random_state=7)\n",
    "z_train_features, z_val_features, z_train_labels, z_val_labels = train_test_split(z_features, z_labels, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVyhCy4S8Gr1"
   },
   "source": [
    "### 1.2.6 Model building\n",
    "We use the function <code>build_model</code>, defined before, to build a model for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IFKAjlnyiaGC"
   },
   "outputs": [],
   "source": [
    "# x_model = build_model(x_features)\n",
    "# y_model = build_model(y_features)\n",
    "# z_model = build_model(z_features)\n",
    "\n",
    "# x_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL_UO_x68ZkV"
   },
   "source": [
    "### 1.2.7 Model fit\n",
    "We fit the model for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vVkJNhUimEV",
    "outputId": "95793237-845e-4603-adab-c9ab15084abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project task_1/tuner/x/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 15:45:46.577512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:46.605149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:46.606027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:46.608756: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-23 15:45:46.610755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:46.611594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:46.612389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:48.788261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:48.789360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:48.789385: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-12-23 15:45:48.790080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-23 15:45:48.791833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1336 MB memory:  -> device: 0, name: NVIDIA GeForce 940M, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #2\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units_1           |176               |?                 \n",
      "units_2           |128               |?                 \n",
      "units_3           |80                |?                 \n",
      "units_4           |48                |?                 \n",
      "learning_rate     |0.001             |?                 \n",
      "tuner/epochs      |2                 |?                 \n",
      "tuner/initial_e...|0                 |?                 \n",
      "tuner/bracket     |2                 |?                 \n",
      "tuner/round       |0                 |?                 \n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 15:45:59.309140: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/604 [======>.......................] - ETA: 18s - loss: 121981.2266 - mean_absolute_error: 256.8960"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# x_model_checkpoint_callback = callbacks.ModelCheckpoint('task_1/models/x', save_weights_only=True, monitor='val_mean_absolute_error', mode='min', save_best_only=True)\n",
    "# y_model_checkpoint_callback = callbacks.ModelCheckpoint('task_1/models/y', save_weights_only=True, monitor='val_mean_absolute_error', mode='min', save_best_only=True)\n",
    "# z_model_checkpoint_callback = callbacks.ModelCheckpoint('task_1/models/z', save_weights_only=True, monitor='val_mean_absolute_error', mode='min', save_best_only=True)\n",
    "\n",
    "# print('---------- x training epochs ----------')\n",
    "# x_history = x_model.fit(x_train_features, x_train_labels, validation_data=(x_val_features, x_val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[x_model_checkpoint_callback])\n",
    "# print('---------- y training epochs ----------')\n",
    "# y_history = y_model.fit(y_train_features, y_train_labels, validation_data=(y_val_features, y_val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[y_model_checkpoint_callback])\n",
    "# print('---------- z training epochs ----------')\n",
    "# z_history = z_model.fit(z_train_features, z_train_labels, validation_data=(z_val_features, z_val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[z_model_checkpoint_callback])\n",
    "\n",
    "tuner = keras_tuner.Hyperband(model_builder,\n",
    "                              objective='val_mean_absolute_error',\n",
    "                              max_epochs=15,\n",
    "                              factor=3,\n",
    "                              directory='task_1/tuner',\n",
    "                              project_name='x')\n",
    "\n",
    "tuner.search(x_train_features, x_train_labels, validation_data=(x_val_features, x_val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWXk115CCw81",
    "outputId": "f58b06ea-b043-40c7-d572-e30b9ab623bc"
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "The hyperparameter search is complete.\n",
    "units_1: {best_hps.get('units_1')}\n",
    "units_2: {best_hps.get('units_2')}\n",
    "units_3: {best_hps.get('units_3')}\n",
    "units_4: {best_hps.get('units_4')}\n",
    "learning_rate: {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKZquHqQyyfc",
    "outputId": "2d383d8a-f3b3-4d85-b088-95eee361cce0"
   },
   "outputs": [],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "x_model = tuner.hypermodel.build(best_hps)\n",
    "x_model.summary()\n",
    "x_model_checkpoint_callback = callbacks.ModelCheckpoint('task_1/models/x', save_weights_only=True, monitor='val_mean_absolute_error', mode='min', save_best_only=True)\n",
    "x_history = x_model.fit(x_train_features, x_train_labels, validation_data=(x_val_features, x_val_labels), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[x_model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnPexhqj8i9R"
   },
   "source": [
    "### 1.2.8 Plotting losses and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reG_A2gHAuy-"
   },
   "outputs": [],
   "source": [
    "x_loss = x_history.history['loss']\n",
    "x_val_loss = x_history.history['val_loss']\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.plot(epochs, x_loss, 'b', label='x training loss')\n",
    "plt.plot(epochs, x_val_loss, 'y', label='x validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MK6fX55rMSIa"
   },
   "outputs": [],
   "source": [
    "# y_loss = y_history.history['loss']\n",
    "# y_val_loss = y_history.history['val_loss']\n",
    "# epochs = range(1, EPOCHS + 1)\n",
    "# plt.plot(epochs, y_loss, 'b', label='y training loss')\n",
    "# plt.plot(epochs, y_val_loss, 'y', label='y validation loss')\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4Gr230LMS6C"
   },
   "outputs": [],
   "source": [
    "# z_loss = z_history.history['loss']\n",
    "# z_val_loss = z_history.history['val_loss']\n",
    "# epochs = range(1, EPOCHS + 1)\n",
    "# plt.plot(epochs, z_loss, 'b', label='z training loss')\n",
    "# plt.plot(epochs, z_val_loss, 'y', label='z validation loss')\n",
    "# plt.title('Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MSE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x49mDpq19Z88"
   },
   "outputs": [],
   "source": [
    "x_mae = x_history.history['mean_absolute_error']\n",
    "x_val_mae = x_history.history['val_mean_absolute_error']\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.plot(epochs, x_mae, 'b', label='x training mae')\n",
    "plt.plot(epochs, x_val_mae, 'y', label='x validation mae')\n",
    "plt.title('MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqoV0kuFMd3w"
   },
   "outputs": [],
   "source": [
    "# y_mae = y_history.history['mean_absolute_error']\n",
    "# y_val_mae = y_history.history['val_mean_absolute_error']\n",
    "# epochs = range(1, EPOCHS + 1)\n",
    "# plt.plot(epochs, y_mae, 'b', label='y training mae')\n",
    "# plt.plot(epochs, y_val_mae, 'y', label='y validation mae')\n",
    "# plt.title('MAE')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9XXd2jrMemL"
   },
   "outputs": [],
   "source": [
    "# z_mae = z_history.history['mean_absolute_error']\n",
    "# z_val_mae = z_history.history['val_mean_absolute_error']\n",
    "# epochs = range(1, EPOCHS + 1)\n",
    "# plt.plot(epochs, z_mae, 'b', label='z training mae')\n",
    "# plt.plot(epochs, z_val_mae, 'y', label='z validation mae')\n",
    "# plt.title('MAE')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9m-FYDF-ifa"
   },
   "source": [
    "### 1.2.9 Evaluation\n",
    "\n",
    "Targets:\n",
    "- x = 81.06\n",
    "- y = 85.26\n",
    "- z = 79.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ3ogayDj3HQ"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('dataset_task_1/test.csv')\n",
    "\n",
    "x_test_data = np.reshape(test_data.x.to_numpy(), (-1, 1))\n",
    "y_test_data = np.reshape(test_data.y.to_numpy(), (-1, 1))\n",
    "z_test_data = np.reshape(test_data.z.to_numpy(), (-1, 1))\n",
    "\n",
    "x_test_features, x_test_labels = features_and_labels(x_test_data, window_size=30, window_shift=6)\n",
    "y_test_features, y_test_labels = features_and_labels(y_test_data, window_size=30, window_shift=6)\n",
    "z_test_features, z_test_labels = features_and_labels(z_test_data, window_size=30, window_shift=6)\n",
    "\n",
    "x_test_features = x_scaler.transform(x_test_features)\n",
    "y_test_features = y_scaler.transform(y_test_features)\n",
    "z_test_features = z_scaler.transform(z_test_features)\n",
    "\n",
    "# Loading the best models for each time series\n",
    "x_model.load_weights('task_1/models/x')\n",
    "# y_model.load_weights('task_1/models/y')\n",
    "# z_model.load_weights('task_1/models/z')\n",
    "\n",
    "x_evaluation = x_model.evaluate(x_test_features, x_test_labels)[1]\n",
    "# y_evaluation = y_model.evaluate(y_test_features, y_test_labels)[1]\n",
    "# z_evaluation = z_model.evaluate(z_test_features, z_test_labels)[1]\n",
    "\n",
    "print(f'x_mean_absolute_error:\\t{x_evaluation}')\n",
    "# print(f'y_mean_absolute_error:\\t{y_evaluation}')\n",
    "# print(f'z_mean_absolute_error:\\t{z_evaluation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9uFsJPTCY3N"
   },
   "source": [
    "### 1.2.10 Predict some values\n",
    "We predict some values to look at how good it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gEmCPgow1Kq"
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "print(f'x')\n",
    "expected_prediction = x_test_labels[index]\n",
    "test = np.reshape(x_test_features[index], (1, x_test_features[index].shape[0]))\n",
    "actual_prediction = x_model.predict(test)[0]\n",
    "print(f'expected prediction:\\t{expected_prediction}')\n",
    "print(f'actual prediction:\\t{actual_prediction}')\n",
    "\n",
    "# print(f'y')\n",
    "# expected_prediction = y_test_labels[index]\n",
    "# test = np.reshape(y_test_features[index], (1, y_test_features[index].shape[0]))\n",
    "# actual_prediction = y_model.predict(test)[0]\n",
    "# print(f'expected prediction:\\t{expected_prediction}')\n",
    "# print(f'actual prediction:\\t{actual_prediction}')\n",
    "\n",
    "# print(f'z')\n",
    "# expected_prediction = z_test_labels[index]\n",
    "# test = np.reshape(z_test_features[index], (1, z_test_features[index].shape[0]))\n",
    "# actual_prediction = z_model.predict(test)[0]\n",
    "# print(f'expected prediction:\\t{expected_prediction}')\n",
    "# print(f'actual prediction:\\t{actual_prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYDqaKyYERkV",
    "tags": []
   },
   "source": [
    "## 1.3 Best window size and window shift selection\n",
    "Now we need to try to find the best <code>window_size</code> and <code>window_shift</code> that better fit the model of one of the time series. We choose the x time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwrljuD_Ew6A"
   },
   "source": [
    "### 1.3.1 Definition of search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vBnfJgYE1wu"
   },
   "outputs": [],
   "source": [
    "oiguworu=egweowubgouew\n",
    "window_sizes = [\n",
    " 18,  # 10 seconds sequences in 3 minutes\n",
    " 24,  # 10 seconds sequences in 4 minutes\n",
    " 30,  # 10 seconds sequences in 5 minutes\n",
    " 36,  # 10 seconds sequences in 6 minutes\n",
    " 42,  # 10 seconds sequences in 7 minutes\n",
    "]\n",
    "\n",
    "window_shifts = [\n",
    " 1,  # 10 seconds shift\n",
    " 3,  # 30 seconds shift\n",
    " 6,  # 1 minute shift\n",
    " 9,  # 1 minute and 30 seconds shift\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPdMHhpMFfMo"
   },
   "source": [
    "### 1.3.2 Model building, fitting and evaluation\n",
    "For each combination, we build the model, fit it, evaluate it and save all the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tm4-C-_21LjY"
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "evaluations = []\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    print(f'window_size: {window_size} - window_shift: {window_shift}')\n",
    "\n",
    "    features, labels = features_and_labels(x_normalized_data, window_size, window_shift)\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.2)\n",
    "    test_features, test_labels = features_and_labels(x_test_normalized_data, window_size=window_size, window_shift=window_shift)\n",
    "\n",
    "    model = build_model(features)\n",
    "\n",
    "    print(f'starting to fit...')\n",
    "    histories.append(model.fit(train_features, train_labels, validation_data=(val_features, val_labels),epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0))\n",
    "    print(f'fit ended!')\n",
    "    print(f'starting to evaluate...')\n",
    "    evaluation = denormalize_mae(model.evaluate(test_features, test_labels), x_scaler)\n",
    "    evaluations.append(evaluation)\n",
    "    print(f'evaluation ended!')\n",
    "    print(f'evaluation: {evaluation}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpTGsMd7Hcic"
   },
   "source": [
    "### 1.3.3 Printing and plotting lossess and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQYd-uoL11Xx"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "maes = []\n",
    "val_maes = []\n",
    "\n",
    "for h in history:\n",
    "  losses.append(h.history['loss'][EPOCHS - 1])\n",
    "  val_losses.append(h.history['val_loss'][EPOCHS - 1])\n",
    "  maes.append(h.history['mean_absolute_error'][EPOCHS - 1])\n",
    "  val_maes.append(h.history['val_mean_absolute_error'][EPOCHS - 1])\n",
    "\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    print(\n",
    "        f'window_size: {window_size} - ' + \n",
    "        f'window_shift: {window_shift} - ' + \n",
    "        f'loss: {losses[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'val_loss: {val_losses[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'mae: {maes[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'val_mae: {val_maes[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - '\n",
    "        )\n",
    "  print()\n",
    "\n",
    "combinations = []\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    combinations.append(f'{window_size}-{window_shift}')\n",
    "\n",
    "plt.bar(combinations, losses, label='Training loss')\n",
    "plt.bar(combinations, val_losses, label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.bar(combinations, maes, label='Training mae')\n",
    "plt.bar(combinations, val_maes, label='Validation mae')\n",
    "plt.title('MAE')\n",
    "plt.xlabel('Combinations')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3ASzEryHpYM"
   },
   "source": [
    "### 1.3.4 Plotting evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9y9cJSN2LLu"
   },
   "outputs": [],
   "source": [
    "plt.bar(combinations, np.reshape(evaluation, 4), label='Evaluation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM-P26ZuskH4",
    "tags": []
   },
   "source": [
    "# DA QUI IN POI LASCIAMO STARE, POI VEDIAMO\n",
    "We use the MinMaxScaler to normalize data in a range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nP_Zu5oJZ7ka",
    "outputId": "ea6ce16a-77ab-442d-c4c1-31e4f62814f8"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruSvW7A4sy4M"
   },
   "source": [
    "### 1.1.3 Data preprocessing\n",
    "We reshape the data in order to have a feature set, containing records of a specific window size, and a label set, containing the next value for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQi36sFwb3Kf",
    "outputId": "3cac925d-dc99-439c-ed8a-8271488ffb78"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "WINDOW_SIZE = 30  # 10 seconds sequences in 5 minutes\n",
    "WINDOW_SHIFT = 6  # 10 seconds sequences in 1 minute\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for i in range(WINDOW_SIZE, len(normalized_data), WINDOW_SHIFT):\n",
    "  features.append(normalized_data[i - WINDOW_SIZE:i])\n",
    "  labels.append(normalized_data[i])\n",
    "features, labels = np.array(features), np.array(labels)\n",
    "\n",
    "print(features[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs5ViXLrzkQR"
   },
   "source": [
    "### 1.1.4 Data splitting\n",
    "We are splitting the data in training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngeSTdMfuvIy"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVwBplk5ugQ6"
   },
   "source": [
    "### 1.1.5 Model definition\n",
    "We defined a sequential model, composed of three LSTM layers, each of 50 units and followed by a Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeftxctfk0e-",
    "outputId": "11abe4f0-ea10-4e9e-995b-4527ed582adb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(name='LSTM_1', units=50, return_sequences=True, input_shape=(features.shape[1], 3)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(units=50, return_sequences=True))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.LSTM(units=50))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units=3))\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss=losses.mse, metrics=[metrics.mae])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0edFrOz_9F"
   },
   "source": [
    "### 1.1.6 Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "id": "AN35kWVO0EZV",
    "outputId": "4bbed59b-32d9-4bbb-9f7e-6edf7dbcbe4a"
   },
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wFVrHg304GQ"
   },
   "source": [
    "### 1.1.7 Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVAbj5milfKI",
    "outputId": "d7e1db00-1533-487a-fde7-a66323e86f32"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "history = model.fit(train_features, train_labels, validation_data=(val_features, val_labels),epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-QddczM0_Wg"
   },
   "source": [
    "### 1.1.8 Plotting losses and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "r7nA8yyIq2TL",
    "outputId": "84a1c19c-f7e7-4bda-ac0f-4e7d736e845b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'go', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "5_3WiIupxg_Z",
    "outputId": "f617d301-eb5f-448f-acf9-4e43c1706762"
   },
   "outputs": [],
   "source": [
    "mae = history.history['mean_absolute_error']\n",
    "val_mae = history.history['val_mean_absolute_error']\n",
    "epochs = range(1, EPOCHS + 1)\n",
    "plt.plot(epochs, mae, 'b', label='Training MAE')\n",
    "plt.plot(epochs, val_mae, 'go', label='Validation MAE')\n",
    "plt.title('MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KYRDeWi1IOr"
   },
   "source": [
    "### 1.1.9 Test evaluation\n",
    "We import the test dataset, then we normalize it, preprocess it and, finally, we evaluate our model over the test dataset\n",
    "\n",
    "x = 81.06\n",
    "y = 85.26\n",
    "z = 79.94\n",
    "\n",
    "(dopo la denormalizzazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hoOegrqoAUt",
    "outputId": "01a78932-4e2e-4b98-ecde-69a540d96a83"
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('dataset_task_1/test.csv')\n",
    "\n",
    "normalized_test_data = scaler.fit_transform(test_data)\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(WINDOW_SIZE, len(normalized_test_data), WINDOW_SHIFT):\n",
    "  test_features.append(normalized_test_data[i - WINDOW_SIZE:i])\n",
    "  test_labels.append(normalized_test_data[i])\n",
    "test_features, test_labels = np.array(test_features), np.array(test_labels)\n",
    "\n",
    "model.evaluate(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5tM0Jki4sBY"
   },
   "source": [
    "### 1.1.10 Test prediction\n",
    "We try to predict some values to check manually how good the prediction is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RURtc0Nq0tbA",
    "outputId": "48ed902c-246a-4ca7-c3b2-6eca3dc18a95"
   },
   "outputs": [],
   "source": [
    "index = 1\n",
    "print(f'expected prediction:\\t{test_labels[index]}')\n",
    "\n",
    "test = np.reshape(test_features[index], (1, test_features[index].shape[0], test_features[index].shape[1]))\n",
    "print(f'actual prediction:\\t{model.predict(test)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_iNHNG8ppnX"
   },
   "source": [
    "## 1.2 Best window size and window shift selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0lJg9eMpo1E",
    "outputId": "f03fe65e-9626-4fa4-942e-073396d34fb7"
   },
   "outputs": [],
   "source": [
    "window_sizes = [\n",
    " 18,  # 10 seconds sequences in 3 minutes\n",
    " 24,  # 10 seconds sequences in 4 minutes\n",
    " 30,  # 10 seconds sequences in 5 minutes\n",
    " 36,  # 10 seconds sequences in 6 minutes\n",
    " 42,  # 10 seconds sequences in 7 minutes\n",
    "]\n",
    "\n",
    "window_shifts = [\n",
    " 3,  # 10  sequences in 30 seconds\n",
    " 6,  # 10 seconds sequences in 1 minute\n",
    " 9,  # 10 seconds sequences in 1 minute and 30 seconds\n",
    "]\n",
    "\n",
    "def features_and_labels(window_size: int = 30, window_shift: int = 6):\n",
    "  WINDOW_SIZE = window_size\n",
    "  WINDOW_SHIFT = window_shift\n",
    "\n",
    "  features = []\n",
    "  labels = []\n",
    "\n",
    "  for i in range(WINDOW_SIZE, len(normalized_data), WINDOW_SHIFT):\n",
    "    features.append(normalized_data[i - WINDOW_SIZE:i])\n",
    "    labels.append(normalized_data[i])\n",
    "  features, labels = np.array(features), np.array(labels)\n",
    "\n",
    "  return features, labels\n",
    "\n",
    "def build_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.LSTM(name='LSTM_1', units=50, return_sequences=True, input_shape=(features.shape[1], 3)))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=50, return_sequences=True))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.LSTM(units=50))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(units=3))\n",
    "  model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss=losses.mse, metrics=[metrics.mae])\n",
    "\n",
    "  return model\n",
    "\n",
    "features, labels = features_and_labels()\n",
    "print(features[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qvM3gK-s8DT",
    "outputId": "9e00862e-26ba-4cc8-89d1-824e0086772f"
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    print(f'window_size: {window_size} - window_shift: {window_shift}')\n",
    "\n",
    "    features, labels = features_and_labels(window_size, window_shift)\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    print(f'starting to fit...')\n",
    "    history.append(model.fit(train_features, train_labels, validation_data=(val_features, val_labels),epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0))\n",
    "    print(f'fit ended!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "id": "cjcGzBSaxaNk",
    "outputId": "1eae0fbe-b4ab-466b-a83b-9ab7e2d0e8b0"
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "maes = []\n",
    "val_maes = []\n",
    "\n",
    "\n",
    "for h in history:\n",
    "  losses.append(h.history['loss'][EPOCHS - 1])\n",
    "  val_losses.append(h.history['val_loss'][EPOCHS - 1])\n",
    "  maes.append(h.history['mean_absolute_error'][EPOCHS - 1])\n",
    "  val_maes.append(h.history['val_mean_absolute_error'][EPOCHS - 1])\n",
    "\n",
    "\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    print(\n",
    "        f'window_size: {window_size} - ' + \n",
    "        f'window_shift: {window_shift} - ' + \n",
    "        f'loss: {losses[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'val_loss: {val_losses[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'mae: {maes[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - ' +\n",
    "        f'val_mae: {val_maes[window_sizes.index(window_size) + window_shifts.index(window_shift)]} - '\n",
    "        )\n",
    "  print()\n",
    "\n",
    "combinations = []\n",
    "for window_size in window_sizes:\n",
    "  for window_shift in window_shifts:\n",
    "    combinations.append(f'{window_size}-{window_shift}')\n",
    "\n",
    "plt.bar(combinations, losses, label='Training loss')\n",
    "plt.bar(combinations, val_losses, label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.bar(combinations, maes, label='Training mae')\n",
    "plt.bar(combinations, val_maes, label='Validation mae')\n",
    "plt.title('MAE')\n",
    "plt.xlabel('Combinations')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4svYqB2vS1S"
   },
   "outputs": [],
   "source": [
    "#valutare tutte le combinazione in base allae predizioni"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ra_xitvr2Lhq",
    "kYDqaKyYERkV",
    "mM-P26ZuskH4",
    "ruSvW7A4sy4M",
    "vs5ViXLrzkQR",
    "kVwBplk5ugQ6",
    "yR0edFrOz_9F",
    "4KYRDeWi1IOr",
    "y5tM0Jki4sBY",
    "B_iNHNG8ppnX"
   ],
   "name": "Deep Learning Project",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
